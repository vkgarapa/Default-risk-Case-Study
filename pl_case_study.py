# -*- coding: utf-8 -*-
"""PL Case Study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142aIlqWCoB2Kc3s7WzpP5tdDcbjCoxuD

<img src="https://paynow7.speedpay.com/PrestigeFinancial/clientimages/PFS-color-logo-small-low-re.jpg" width="520">

The following Case study is sponsored by prestige finacial.

# Importing Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing packages
import pandas as pd
import numpy as np
import time
import gc
from collections import Counter

# Importing plotting packages
import numpy as np
import plotly.offline as py
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')
import plotly.express as px
import plotly.graph_objects as go


# Sampling Packages
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
from mlxtend.plotting import plot_learning_curves
from yellowbrick.model_selection import LearningCurve
from yellowbrick.features.importances import FeatureImportances
!pip install impyute
import impyute

# A host of Scikit-learn models
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.kernel_approximation import Nystroem
from sklearn.kernel_approximation import RBFSampler
from sklearn.pipeline import make_pipeline

!pip install bayesian-optimization
from bayes_opt import BayesianOptimization
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split,StratifiedKFold, learning_curve, validation_curve,KFold
from sklearn.metrics import auc,confusion_matrix
import xgboost as xgb
from sklearn.metrics import roc_auc_score,roc_curve, precision_score, recall_score, f1_score, accuracy_score
from scipy.stats import ranksums


import warnings
warnings.simplefilter(action = 'ignore')

def reduce_mem_usage(data, verbose = True):
    start_mem = data.memory_usage().sum() / 1024**2
    if verbose:
        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))
    
    for col in data.columns:
        col_type = data[col].dtype
        
        if col_type != object:
            c_min = data[col].min()
            c_max = data[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    data[col] = data[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    data[col] = data[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    data[col] = data[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    data[col] = data[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    data[col] = data[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    data[col] = data[col].astype(np.float32)
                else:
                    data[col] = data[col].astype(np.float64)

    end_mem = data.memory_usage().sum() / 1024**2
    if verbose:
        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))
        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return data

"""# Data Compilation"""

train = pd.read_csv('/content/application_train.csv')
test = pd.read_csv('/content/application_test.csv')

import time

"""## Data Understanding"""

print(train.info())

print("Number of numerical columns: 106")
print("number of categorical columns: 16")

train.describe()

test.info()

test.describe()

num = train.select_dtypes(['float64', 'int64'])
num_test = test.select_dtypes(['float64', 'int64'])
num.head()

num_test.head()

"""## Insights of the Data

#### Frequency plot of Target variable
"""

plt.figure(figsize=[10,5])
sns.countplot(train['TARGET'], data= train)
print(train['TARGET'].value_counts())

"""#### Box and Dist plots of the data"""

sns.distplot(train[train['AMT_INCOME_TOTAL'] < 2000000]['AMT_INCOME_TOTAL'])

(train[train['AMT_INCOME_TOTAL'] > 1000000]['TARGET'].value_counts())/len(train[train['AMT_INCOME_TOTAL'] > 1000000])*100

sns.boxplot(train['AMT_INCOME_TOTAL'] )

sns.distplot(train['AMT_CREDIT'])

sns.distplot(np.log(train['AMT_CREDIT']))

sns.boxplot(train['AMT_CREDIT'])

sns.boxplot(train['AMT_ANNUITY'])

sns.boxplot(train['AMT_GOODS_PRICE'])

sns.boxplot(train['DAYS_BIRTH'])

sns.distplot(train['DAYS_BIRTH']/(-365))
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Count of people')

sns.boxplot(train['DAYS_EMPLOYED'])

# Replace the error values with nan
train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)

sns.distplot((abs(train['DAYS_EMPLOYED'])/365), bins = 50)
plt.title('Years before the application the person started current employment')
plt.xlabel('Years of employment')
plt.ylabel('Count of people')

train[train['DAYS_EMPLOYED']>(-365*2)]['TARGET'].value_counts()/sum(train['DAYS_EMPLOYED']>(-365*2))

sns.boxplot(train['DAYS_REGISTRATION'])

sns.boxplot(train['DAYS_ID_PUBLISH'])

sns.boxplot(train['OWN_CAR_AGE'] )

sns.boxplot(train['HOUR_APPR_PROCESS_START'])

plt.figure(figsize = [12,7])
sns.boxplot(train['DAYS_EMPLOYED'])

sns.boxplot(train['REGION_RATING_CLIENT_W_CITY'] )

sns.boxplot(train['AMT_REQ_CREDIT_BUREAU_QRT'] )

sns.boxplot(train['OBS_30_CNT_SOCIAL_CIRCLE'] )

"""#### Frequency plots of categorical variables"""

#train.select_dtypes('object').columns

sns.countplot(x=train['NAME_CONTRACT_TYPE'],hue=train['TARGET'])

sns.countplot(x=train['CODE_GENDER'],hue=train['TARGET'], data=train)

sns.countplot(x=train['FLAG_OWN_CAR'],hue=train['TARGET'])

sns.countplot(x=train['FLAG_OWN_REALTY'],hue=train['TARGET'])

plt.figure(figsize=[12,7])
sns.countplot(x=train['NAME_TYPE_SUITE'],hue=train['TARGET'])

plt.figure(figsize=[12,7])
sns.countplot(x=train['NAME_INCOME_TYPE'],hue=train['TARGET'])

plt.figure(figsize=[12,7])
sns.countplot(x=train['NAME_EDUCATION_TYPE'],hue=train['TARGET'])

plt.figure(figsize=[12,7])
sns.countplot(x=train['NAME_FAMILY_STATUS'],hue=train['TARGET'])

plt.figure(figsize=[12,7])
sns.countplot(x=train['NAME_HOUSING_TYPE'],hue=train['TARGET'])

plt.figure(figsize=[30,7])
sns.countplot(x=train['OCCUPATION_TYPE'],hue=train['TARGET'])

plt.figure(figsize=[100,7])
sns.countplot(x=train['ORGANIZATION_TYPE'],hue=train['TARGET'])

sns.countplot(x=train['FONDKAPREMONT_MODE'],hue=train['TARGET'])

sns.countplot(x=train['HOUSETYPE_MODE'],hue=train['TARGET'])

sns.countplot(x=train['WALLSMATERIAL_MODE'],hue=train['TARGET'])

sns.countplot(x=train['EMERGENCYSTATE_MODE'],hue=train['TARGET'])

sns.countplot(x=train['WEEKDAY_APPR_PROCESS_START'],hue=train['TARGET'])

"""# Data Preparation

## Imputing NULL Values
"""

num.drop(columns=['TARGET'],axis=1, inplace=True)
#num_test.drop(columns=['SK_ID_CURR'],axis=1, inplace=True)

na = pd.DataFrame(num.isnull().sum())
na.rename(columns = {0: 'Sum of NULL values'}, inplace=True)
na['Percentage'] = (num.isnull().sum()/len(num)*100)
#print(na[na['Sum of NULL values'] >0].to_string())
cols = na[na['Sum of NULL values'] >0].index.to_list()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
#   df = num[cols]
#   df = impyute.imputation.cs.median(df)
#   df.columns = cols
#   df.isnull().sum() 
#   df.head()

train.drop(columns = df.columns, axis=1, inplace=True)
train = pd.concat([train,df],axis=1)

train.info()

na = pd.DataFrame(num_test.isnull().sum())
na.rename(columns = {0: 'Sum of NULL values'}, inplace=True)
na['Percentage'] = (num_test.isnull().sum()/len(num_test)*100)
#print(na[na['Sum of NULL values'] >0].to_string())
cols = na[na['Sum of NULL values'] >0].index.to_list()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
#   df = num_test[cols]
#   df = impyute.imputation.cs.median(df)
#   df.columns = cols
#   df.isnull().sum() 
#   df.head()

test.drop(columns = df.columns, axis=1, inplace=True)
test = pd.concat([test,df],axis=1)

test.info()

cats = train.select_dtypes('object')
cats.head()

cats_test = test.select_dtypes('object')
cats_test.head()

cats.isnull().sum()

def cat_fill(df):
  df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].fillna('Unknown')
  df['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].fillna('Unknown')
  df['FONDKAPREMONT_MODE'] = df['FONDKAPREMONT_MODE'].fillna('Unknown')
  df['HOUSETYPE_MODE'] = df['HOUSETYPE_MODE'].fillna('Unknown')
  df['WALLSMATERIAL_MODE'] = df['WALLSMATERIAL_MODE'].fillna('Unknown')
  df['EMERGENCYSTATE_MODE'] = df['EMERGENCYSTATE_MODE'].fillna('Unknown')

  return cats.head()

cat_fill(cats)

cat_fill(cats_test)

cats.isnull().sum()

cats_test.isnull().sum()

train.drop(columns=cats.columns, axis=1, inplace=True)
train = pd.concat([train, cats],axis=1)

test.drop(columns=cats_test.columns, axis=1, inplace=True)
test = pd.concat([test, cats_test],axis=1)

"""## Feature engineering"""

#train.select_dtypes(['float64', 'int64']).columns.to_list()

#train.select_dtypes(['object']).columns.to_list()

def new_fea(df):

  # Some simple new features based on my understanding of the data
  df['AMT_LEFT_EXP'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE'] - df['AMT_ANNUITY']
  df['AMT_INCOME_TOTAL - AMT_GOODS_PRICE'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE']

  df['CNT_adults'] = df['CNT_FAM_MEMBERS'] - df['CNT_CHILDREN']
  df['income_contrib'] = df['AMT_INCOME_TOTAL'] / df['CNT_adults']
  df['days_employed_perc'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']
  df['days_education'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']

  df['credit_minus_goods'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']
  df['credit_div_goods'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']
  df['annuity_length'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']
  df['AMT_CREDIT / AMT_INCOME_TOTAL'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']
  df['income_credit_perc'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']
  df['income_per_person'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']

  df['AMT_INCOME_TOTAL / 12 - AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']
  df['AMT_INCOME_TOTAL / AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / df['AMT_ANNUITY']

  df['annuity_income_perc'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])
  df['DAYS_REGISTRATION / DAYS_ID_PUBLISH'] = df['DAYS_REGISTRATION'] / df['DAYS_ID_PUBLISH']
  df['ann_len_emply_ratio'] = df['annuity_length'] / df['DAYS_EMPLOYED']
  df['children_ratio'] = df['CNT_CHILDREN'] / df['CNT_FAM_MEMBERS']

  df['reg_div_publish'] = df['DAYS_REGISTRATION'] / df['DAYS_ID_PUBLISH']
  df['birth_div_reg'] = df['DAYS_BIRTH'] / df['DAYS_REGISTRATION']
  df['flag_document_sum'] = df['FLAG_DOCUMENT_2'] + df['FLAG_DOCUMENT_3'] + df['FLAG_DOCUMENT_4'] + df['FLAG_DOCUMENT_5'] + df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_7'] + df['FLAG_DOCUMENT_8'] + df['FLAG_DOCUMENT_9'] + df['FLAG_DOCUMENT_10'] + df['FLAG_DOCUMENT_11'] + df['FLAG_DOCUMENT_12'] + df['FLAG_DOCUMENT_13'] + df['FLAG_DOCUMENT_14'] + df['FLAG_DOCUMENT_15'] + df['FLAG_DOCUMENT_16'] + df['FLAG_DOCUMENT_17'] + df['FLAG_DOCUMENT_18'] + df['FLAG_DOCUMENT_19'] + df['FLAG_DOCUMENT_20'] + df['FLAG_DOCUMENT_21']
  df['is_na_amt_annuity'] = 1.0*np.isnan(df['AMT_ANNUITY'])
  df['age_finish'] = df['DAYS_BIRTH']*(-1.0/365) + (df['AMT_CREDIT']/df['AMT_ANNUITY']) *(1.0/12)
  df['new_inc_per_chld'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])

  df['new_car_to_birth_ratio'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']
  df['new_car_to_employ_ratio'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']
  df['new_phone_to_birth_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']
  df['new_phone_to_birth_ratio_employer'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']
  

  # Creating features based on popularity ()  
  df['most popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \
                                                  .isin([225000, 450000, 675000, 900000]).map({True: 1, False: 0})
  df['popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \
                                                  .isin([1125000, 1350000, 1575000, 1800000, 2250000]).map({True: 1, False: 0})
  df['fea_missing'] = df.isnull().sum(axis = 1).values

  return reduce_mem_usage(df), df.head()

def new_insights_fea(df):

  # New features are added after evaluating outliers, nulls and various plots
  # Remove some empty features, these features were selected based from the corr plot and observing data
  df.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 
            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 
            'FLAG_DOCUMENT_21'], axis = 1, inplace = True)
  
  # Replacing outliers, the following metrics were obtained after extensive plotting of boxplots
  df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)
  df.loc[df['OWN_CAR_AGE'] > 80, 'OWN_CAR_AGE'] = np.nan
  df.loc[df['REGION_RATING_CLIENT_W_CITY'] < 0, 'REGION_RATING_CLIENT_W_CITY'] = np.nan
  df.loc[df['AMT_INCOME_TOTAL'] > 1e8, 'AMT_INCOME_TOTAL'] = np.nan


  # Some more additional features by data aggregation 
  df['EXT_SOURCE_mean'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)
  df['EXT_SOURCE_std'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis = 1)
  df['EXT_SOURCE_prod'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']
  df['EXT_SOURCE_1 * EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']
  df['EXT_SOURCE_1 * EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']
  df['EXT_SOURCE_2 * EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']
  df['EXT_SOURCE_1 * DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']
  df['EXT_SOURCE_2 * DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']
  df['EXT_SOURCE_3 * DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']
  df['EXT_SOURCE_1 / DAYS_BIRTH'] = df['EXT_SOURCE_1'] / df['DAYS_BIRTH']
  df['EXT_SOURCE_2 / DAYS_BIRTH'] = df['EXT_SOURCE_2'] / df['DAYS_BIRTH']
  df['EXT_SOURCE_3 / DAYS_BIRTH'] = df['EXT_SOURCE_3'] / df['DAYS_BIRTH']

  return reduce_mem_usage(df), df.head()

new_fea(train)

new_insights_fea(train)

train.info()

new_fea(test)

new_insights_fea(test)

test.info()

def dummy(data):
  dummy = pd.get_dummies(data.select_dtypes('object'), drop_first=True)

  data.drop(columns = data.select_dtypes('object'), axis=1, inplace=True)
  data = pd.concat([data,dummy],axis=1)
  
  return data.head()

dummy(train)

train.info()

dummy(test)

test.info()

train.head()

"""## Data Cleaning"""

def corr_feature_with_target(feature, target):
    c0 = feature[target == 0].dropna()
    c1 = feature[target == 1].dropna()
        
    if set(feature.unique()) == set([0, 1]):
        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))
    else:
        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))
        
    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2
        
    return [diff, p]

def clean_data(data):
    warnings.simplefilter(action = 'ignore')
    
    # Removing empty features
    nun = data.nunique()
    empty = list(nun[nun <= 1].index)
    
    data.drop(empty, axis = 1, inplace = True)
    print('After removing empty features there are {0:d} features'.format(data.shape[1]))
    
    # Removing features with the same distribution on 0 and 1 classes
    corr = pd.DataFrame(index = ['diff', 'p'])
    ind = data[data['TARGET'].notnull()].index
    
    for c in data.columns.drop('TARGET'):
        corr[c] = corr_feature_with_target(data.loc[ind, c], data.loc[ind, 'TARGET'])

    corr = corr.T
    corr['diff_norm'] = abs(corr['diff'] / data.mean(axis = 0))
    
    to_del_1 = corr[((corr['diff'] == 0) & (corr['p'] > .05))].index
    to_del_2 = corr[((corr['diff_norm'] < .5) & (corr['p'] > .05))].drop(to_del_1).index
    to_del = list(to_del_1) + list(to_del_2)
    if 'SK_ID_CURR' in to_del:
        to_del.remove('SK_ID_CURR')
        
    data.drop(to_del, axis = 1, inplace = True)
    print('After removing features with the same distribution on 0 and 1 classes there are {0:d} features'.format(data.shape[1]))
    
    # Removing features with not the same distribution on train and test datasets
    corr_test = pd.DataFrame(index = ['diff', 'p'])
    target = data['TARGET'].notnull().astype(int)
    
    for c in data.columns.drop('TARGET'):
        corr_test[c] = corr_feature_with_target(data[c], target)

    corr_test = corr_test.T
    corr_test['diff_norm'] = abs(corr_test['diff'] / data.mean(axis = 0))
    
    bad_features = corr_test[((corr_test['p'] < .05) & (corr_test['diff_norm'] > 1))].index
    bad_features = corr.loc[bad_features][corr['diff_norm'] == 0].index
    
    data.drop(bad_features, axis = 1, inplace = True)
    print('After removing features with not the same distribution on train and test datasets there are {0:d} features'.format(data.shape[1]))
    
    del corr, corr_test
    gc.collect()
    
    # Removing features not interesting for classifier
    clf = LGBMClassifier(random_state = 0)
    train_index = data[data['TARGET'].notnull()].index
    train_columns = data.drop('TARGET', axis = 1).columns

    score = 1
    new_columns = []
    while score > .7:
        train_columns = train_columns.drop(new_columns)
        clf.fit(data.loc[train_index, train_columns], data.loc[train_index, 'TARGET'])
        f_imp = pd.Series(clf.feature_importances_, index = train_columns)
        score = roc_auc_score(data.loc[train_index, 'TARGET'], 
                              clf.predict_proba(data.loc[train_index, train_columns])[:, 1])
        new_columns = f_imp[f_imp > 0].index

    data.drop(train_columns, axis = 1, inplace = True)
    print('After removing features not interesting for classifier there are {0:d} features'.format(data.shape[1]))

    return data

train1 = df = pd.concat([train, test], axis = 0, ignore_index = True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train1 = clean_data(train1)

train.info()

test.info()

"""## Data Sample"""

X = train.drop(columns=['TARGET'],axis=1)
y = train['TARGET']
#y = y.values.reshape(-1,1)
print(X.shape, y.shape)

# Dropping the following columns as these features are not present in the test data set and cause key error
#X.drop(columns=['NAME_FAMILY_STATUS_Unknown', 'NAME_INCOME_TYPE_Maternity leave', 'CODE_GENDER_XNA'],axis=1, inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.15, random_state=123)

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1001)

"""# Data Modelling

## Custom functions for model building

### Ploting ROC function
"""

def plot_roc_curve(fpr, tpr):
  fig = go.Figure()
  fig.add_trace(go.Scatter(x=fpr, y=tpr,
                    mode='lines',
                    name='ROC Curve'))
  fig.add_trace(go.Scatter(x=[0,1], y=[0,1],
                    mode='lines+markers',
                    name='Baseline'))

  fig.update_layout(
    title="Receiver operating characteristic",
    xaxis_title="False Positive Rate",
    yaxis_title="True Positive Rate",
    font=dict(
        family="Courier New, monospace",
        size=18,
        color="#7f7f7f"
    )
)

  fig.show()

"""### Feature importances plot"""

def feature_imp(X, model):

  p = model.feature_importances_
  q = X.columns.values
  data = [go.Bar(
            x= q,
             y= p, 
             orientation='v',
            width = 0.5,
            marker=dict(
               color = model.feature_importances_,
            colorscale='Portland',
            showscale=True,
            reversescale = False
            ),
            opacity=0.6
        )]

  layout= go.Layout(
    autosize= True,
    title= 'Barplots of XGb-Classification Feature Importance',
    hovermode= 'closest',
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
          )
  fig = go.Figure(data=data, layout=layout)
  py.iplot(fig, filename='bar-direct-labels')



"""### Learning Curve"""

def plot_learning_curve(train_sizes,train_scores, test_scores):

  # Create means and standard deviations of training set scores
  train_mean = np.mean(train_scores, axis=1)
  train_std = np.std(train_scores, axis=1)

  # Create means and standard deviations of test set scores
  test_mean = np.mean(test_scores, axis=1)
  test_std = np.std(test_scores, axis=1)

  fig = go.Figure()
  fig.add_trace(go.Scatter(x=train_sizes, y=train_mean,
                    mode='lines', line_color = 'rgb(111, 231, 219)',
                    name='Training score'))
  fig.add_trace(go.Scatter(x=train_sizes, y=test_mean,
                    mode='lines+markers',
                    name='Cross-validation score',fill='tonexty',line_color='indigo'))

  fig.update_layout(
    title="Learning Curve",
    xaxis_title="Training Set Size",
    yaxis_title="Auc Score",
    font=dict(
        family="Courier New, monospace",
        size=18,
        color="#7f7f7f")
        )
  fig.show()

"""### LGBM Hyperparameter tuning"""

import lightgbm

param_lgb_df = pd.DataFrame()

# Commented out IPython magic to ensure Python compatibility.
def lgbm_hyper(X1_train, y1_test):

  train_data = lightgbm.Dataset(data=X1_train, label=y1_test)

  def lgbm_eval(num_leaves,colsample_bytree,subsample,max_depth,reg_alpha,reg_lambda,min_split_gain,min_child_weight,):
    params = {'application':'binary','num_iterations':4000, 'learning_rate':0.05, 'early_stopping_round':100, 'metric':'auc'}
    params["num_leaves"] = int(num_leaves)
    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)
    params['subsample'] = max(min(subsample, 1), 0)
    params['max_depth'] = int(max_depth)
    params['reg_alpha'] = max(reg_alpha, 0)
    params['reg_lambda'] = max(reg_lambda, 0)
    params['min_split_gain'] = min_split_gain
    params['min_child_weight'] = min_child_weight



    clf = lightgbm.cv(params, train_data, nfold=5, seed=32587, stratified=True, verbose_eval =200, metrics=['auc'])

    return clf['auc-mean'][-1]


  clf_bo = BayesianOptimization(lgbm_eval, {'num_leaves': (30, 45),
                                          'colsample_bytree': (0.1, 1),
                                          'subsample': (0.1, 1),
                                          'max_depth': (5, 15),
                                          'reg_alpha': (0, 10),
                                          'reg_lambda': (0, 10),
                                          'min_split_gain': (0, 1),
                                          'min_child_weight': (30, 45),
                                        })

#   %%time
  import warnings
  with warnings.catch_warnings():
    warnings.filterwarnings('ignore')
    clf_bo.maximize(init_points=5, n_iter=20, acq='ei', xi=0.0)

  
  # Saving the scores to a dataframe
  param_lgb_df = pd.DataFrame.from_dict( clf_bo.res)
  param_lgb_df['AUC'] = ( param_lgb_df['target'] + 1 ) / 2
  param_lgb_df = param_lgb_df[['params','AUC']]
  param_lgb_df.to_csv('PF-AUC-5fold-LGBM-run-01-grid.csv')


  global param_lgb_model

  param_lgb_model= clf_bo.max

  return param_lgb_model, param_lgb_df

lgbm_hyper(X_train, y_train)

lgbm_params = param_lgb_model

lgbm_params

"""### XGB Hyperparameter tuning"""

param_df = pd.DataFrame()

AUCbest = -1.
ITERbest = 0

# Commented out IPython magic to ensure Python compatibility.
def xgb_hyper(X1_train, y1_train):


  def XGB_CV(max_depth,gamma,min_child_weight,max_delta_step,subsample,colsample_bytree):
    global AUCbest
    global ITERbest


    # Define all XGboost parameters
    #

    paramt = {
              'booster' : 'gbtree',
              'max_depth' : int(max_depth),
              'gamma' : gamma,
              'eta' : 0.1,
              'objective' : 'binary:logistic',
              'nthread' : 4,
              'silent' : True,
              'eval_metric': 'auc',
              'subsample' : max(min(subsample, 1), 0),
              'colsample_bytree' : max(min(colsample_bytree, 1), 0),
              'min_child_weight' : min_child_weight,
              'max_delta_step' : int(max_delta_step),
              'seed' : 32587
              }

    folds = 5
    cv_score = 0

    print("\n Search parameters (%d-fold validation):\n %s" % (folds, paramt), file=log_file )
    log_file.flush()

    xgbc = xgb.cv(paramt,dtrain,num_boost_round = 20000,stratified = True,nfold = folds,early_stopping_rounds = 100,metrics = 'auc',
                 show_stdv = True)

    val_score = xgbc['test-auc-mean'].iloc[-1]
    train_score = xgbc['train-auc-mean'].iloc[-1]
    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),
    (val_score*2-1)) )
    if ( val_score > AUCbest ):
      AUCbest = val_score
      ITERbest = len(xgbc)

    return (val_score*2) - 1

  
  # Define the log file. If you repeat this run, new output will be added to it
  log_file = open('PF-AUC-5fold-XGB-run-01-v1-full.log', 'a')
  AUCbest = -1
  ITERbest = 0

  # Converting the dataset to dmatrix for faster calculation
  dtrain = xgb.DMatrix(X1_train, label = y1_train)


  # Setting Bayesian optimization tunning
  XGB_BO = BayesianOptimization(XGB_CV, {'max_depth': (2, 40),
                                        'gamma': (0.001, 10.0),
                                        'min_child_weight': (0, 20),
                                        'max_delta_step': (0, 10),
                                        'subsample': (0.4, 1.0),
                                        'colsample_bytree' :(0.4, 1.0)
                                        })
  

  
  print('-'*130)
  print('-'*130, file=log_file)
  log_file.flush()

  
  import warnings
  with warnings.catch_warnings():
    warnings.filterwarnings('ignore')
    XGB_BO.maximize(init_points=5, n_iter=20, acq='ei', xi=0.0)

#   %%time
  # Saving the scores to a dataframe
  param_df = pd.DataFrame.from_dict(XGB_BO.res)
  param_df['AUC'] = ( param_df['target'] + 1 ) / 2
  param_df = param_df[['params','AUC']]
  param_df.to_csv('AUC-5fold-grid.csv')


  global param_model

  param_model= XGB_BO.max

  return param_model, param_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgb_hyper(X_train, y_train)

xgb_params = param_model

xgb_params

"""### Training LGBM model

'params': {'colsample_bytree': 0.11463761511704018,
   'max_depth': 5.671205833879625,
   'min_child_weight': 30.045183337391244,
   'min_split_gain': 0.5682051221202666,
   'num_leaves': 44.954940915304014,
   'reg_alpha': 0.09397781838370367,
   'reg_lambda': 0.3411955743109629,
   'subsample': 0.5127931493051976},
  'target': 0.7580072710560077}
"""



def cv_lightgbm(df, num_folds, stratified = False, debug= False):

    # Reducing the size of the file
    df = reduce_mem_usage(df)

    # Divide in training/validation and test data
    train_df = df[df['TARGET'].notnull()]
    test_df = df[df['TARGET'].isnull()]
    print("Starting LightGBM. Train shape: {}, test shape: {}".format(train_df.shape, test_df.shape))

    gc.collect()
    # Cross validation model
    if stratified:
        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=101)
    else:
        folds = KFold(n_splits= num_folds, shuffle=True, random_state=101)

    # Create arrays and dataframes to store results
    oof_preds = np.zeros(train_df.shape[0])
    sub_preds = np.zeros(test_df.shape[0])
    feature_importance_df = pd.DataFrame()
    feats = [f for f in train_df.columns if f not in ['SK_ID_CURR','TARGET']]
    
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):
        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]
        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]

        # LightGBM parameters found by Bayesian optimization
        clf = LGBMClassifier(
            nthread=4,
            n_estimators=10000,
            learning_rate=0.02,
            num_leaves=45,
            #colsample_bytree=0.9497036,
            feature_fraction=0.3,
            subsample=0.5127931493051976,
            max_depth=6,
            reg_alpha=0.09397781838370367,
            reg_lambda=0.3411955743109629,
            min_split_gain=0.5682051221202666,
            min_child_weight=30,
            silent=1,
            verbose=-1 )

        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], 
            eval_metric= 'auc', verbose= False, early_stopping_rounds= 200)

        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]
        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits

        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = feats
        fold_importance_df["importance"] = clf.feature_importances_
        fold_importance_df["fold"] = n_fold + 1
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))
        del clf, train_x, train_y, valid_x, valid_y
        gc.collect()

    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))
    # Write submission file and plot feature importance
    if not debug:
        test_df['TARGET'] = sub_preds
        test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission_lgbm_1.csv', index= False)

    #sc = print(roc_auc_score(train_df['TARGET'], oof_preds))
    #feature_imp(train_df,clf)
    return sc

cv_lightgbm(train1, num_folds = 5, stratified = True, debug= False)



"""### Training Xgboost model"""

def cv_xgb(df, num_folds, stratified = False, debug= False):

    # Reducing the size of the file
    df = reduce_mem_usage(df)

    # Divide in training/validation and test data
    train_df = df[df['TARGET'].notnull()]
    test_df = df[df['TARGET'].isnull()]
    print("Starting XGB. Train shape: {}, test shape: {}".format(train_df.shape, test_df.shape))

    gc.collect()
    # Cross validation model
    if stratified:
        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=101)
    else:
        folds = KFold(n_splits= num_folds, shuffle=True, random_state=101)

    # Create arrays and dataframes to store results
    oof_preds = np.zeros(train_df.shape[0])
    sub_preds = np.zeros(test_df.shape[0])
    feature_importance_df = pd.DataFrame()
    feats = [f for f in train_df.columns if f not in ['SK_ID_CURR','TARGET']]
    
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):
        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]
        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]

        # LightGBM parameters found by Bayesian optimization
        xgb_params = {
                              'seed': 0,
                              'colsample_bytree': 0.7,
                              'silent': 1,
                              'subsample': 0.7,
                              'learning_rate': 0.075,
                              'objective': 'binary:logistic',
                              'max_depth': 4,
                              'num_parallel_tree': 1,
                              'min_child_weight': 1,
                              'nrounds': 200
                          }
        

        clf = XGBClassifier(nthread=4,n_estimators=10000,learning_rate=0.02, params=xgb_params,eval_metric= 'auc', verbose= False, early_stopping_rounds= 200)

        clf.fit(train_x, train_y)

        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]
        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits

        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = feats
        fold_importance_df["importance"] = clf.feature_importances_
        fold_importance_df["fold"] = n_fold + 1
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))
        del clf, train_x, train_y, valid_x, valid_y
        gc.collect()

    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))
    # Write submission file and plot feature importance
    if not debug:
        test_df['TARGET'] = sub_preds
        test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission_xgb_1.csv', index= False)
        
    #xc = print(roc_auc_score(train_df['TARGET'], oof_preds))
    #imp = feature_imp(train_df,clf)
    return feature_importance_df

cv_xgb(train1, num_folds= 5, stratified = True, debug= False)

"""### Stacking

Experimenting with stacking. 

NOTE: I do not have experience with stacking. I refrenced https://www.dataquest.io/blog/introduction-to-ensembles/ article. it was a hands on experience but lacked the knowledge to excute it effectively

#!pip install catboost
from catboost import CatBoostClassifier
from sklearn.ensemble import ExtraTreesClassifier
"""

#ds = pd.read_csv('/content/application_train.csv')

#dt = train.copy()

"""
trs = train[:125000]
trs.drop(columns=['TARGET'], axis=1, inplace=True)
x_train1 = trs
tss = train[125000:]
tss.drop(columns=['TARGET'], axis=1, inplace=True)
X_test1 = tss
y_train1 = ds['TARGET']
ntrain = x_train1.shape[0]
ntest = x_test1.shape[0]
"""

#x_train1

"""class SklearnWrapper(object):
    def __init__(self, clf, seed=0, params=None):
        params['random_state'] = seed
        self.clf = clf(**params)

    def train(self, x_train1, y_train1):
        self.clf.fit(x_train1, y_train1)

    def predict(self, x):
        return self.clf.predict_proba(x)[:,1]

class CatboostWrapper(object):
    def __init__(self, clf, seed=0, params=None):
        params['random_seed'] = seed
        self.clf = clf(**params)

    def train(self, x_train1, y_train1):
        self.clf.fit(x_train1, y_train1)

    def predict(self, x):
        return self.clf.predict_proba(x)[:,1]
        
class LightGBMWrapper(object):
    def __init__(self, clf, seed=0, params=None):
        params['feature_fraction_seed'] = seed
        params['bagging_seed'] = seed
        self.clf = clf(**params)

    def train(self, x_train1, y_train1):
        self.clf.fit(x_train1, y_train1)

    def predict(self, x):
        return self.clf.predict_proba(x)[:,1]


class XgbWrapper(object):
    def __init__(self, seed=0, params=None):
        self.param = params
        self.param['seed'] = seed
        self.nrounds = params.pop('nrounds', 250)

    def train(self, x_train1, y_train1):
        dtrain = xgb.DMatrix(x_train1, label=y_train1)
        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)

    def predict(self, x):
        return self.gbdt.predict(xgb.DMatrix(x))

def get_oof(clf):
    oof_train = np.zeros((ntrain,))
    oof_test = np.zeros((ntest,))
    oof_test_skf = np.empty((NFOLDS, ntest))

    for i, (train_index, test_index) in enumerate(kf.split(x_train1)):
        x_tr = x_train1.loc[train_index]
        y_tr = y_train1.loc[train_index]
        x_te = x_train1.loc[test_index]

        clf.train(x_tr, y_tr)

        oof_train[test_index] = clf.predict(x_te)
        oof_test_skf[i, :] = clf.predict(x_test1)

    oof_test[:] = oof_test_skf.mean(axis=0)
    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)

et_params = {
    'n_jobs': 16,
    'n_estimators': 200,
    'max_features': 0.5,
    'max_depth': 12,
    'min_samples_leaf': 2,
}

rf_params = {
    'n_jobs': 16,
    'n_estimators': 200,
    'max_features': 0.2,
    'max_depth': 12,
    'min_samples_leaf': 2,
}

xgb_params = {
    'seed': 0,
    'colsample_bytree': 0.7,
    'silent': 1,
    'subsample': 0.7,
    'learning_rate': 0.075,
    'objective': 'binary:logistic',
    'max_depth': 4,
    'num_parallel_tree': 1,
    'min_child_weight': 1,
    'nrounds': 200
}

catboost_params = {
    'iterations': 200,
    'learning_rate': 0.5,
    'depth': 3,
    'l2_leaf_reg': 40,
    'bootstrap_type': 'Bernoulli',
    'subsample': 0.7,
    'scale_pos_weight': 5,
    'eval_metric': 'AUC',
    'od_type': 'Iter',
    'allow_writing_files': False
}

lightgbm_params = {
    'n_estimators':200,
    'learning_rate':0.1,
    'num_leaves':123,
    'colsample_bytree':0.8,
    'subsample':0.9,
    'max_depth':15,
    'reg_alpha':0.1,
    'reg_lambda':0.1,
    'min_split_gain':0.01,
    'min_child_weight':2    
}

NFOLDS = 5
SEED = 32587 # Random number
kf = KFold(n_splits = NFOLDS, shuffle=True, random_state=SEED)

xg = XgbWrapper(seed=SEED, params=xgb_params)
et = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)
rf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)
cb = CatboostWrapper(clf= CatBoostClassifier, seed = SEED, params=catboost_params)
lg = LightGBMWrapper(clf = LGBMClassifier, seed = SEED, params = lightgbm_params)

xg_oof_train, xg_oof_test = get_oof(xg)
et_oof_train, et_oof_test = get_oof(et)
rf_oof_train, rf_oof_test = get_oof(rf)
cb_oof_train, cb_oof_test = get_oof(cb)

print("XG-CV: {}".format(sqrt(mean_squared_error(y_train, xg_oof_train))))
print("ET-CV: {}".format(sqrt(mean_squared_error(y_train, et_oof_train))))
print("RF-CV: {}".format(sqrt(mean_squared_error(y_train, rf_oof_train))))
print("RF-CV: {}".format(sqrt(mean_squared_error(y_train, cb_oof_train))))

x_train1 = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)
x_test1 = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)

print("{},{}".format(x_train1.shape, x_test1.shape))

logistic_regression = LogisticRegression()
logistic_regression.fit(x_train1,y_train1)

test['TARGET'] = logistic_regression.predict_proba(x_test1)[:,1]

test[['SK_ID_CURR', 'TARGET']].to_csv('stacking_submission.csv', index=False, float_format='%.8f')
"""

